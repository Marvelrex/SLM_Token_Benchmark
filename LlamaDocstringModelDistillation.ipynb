{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install codecarbon\n",
        "!pip uninstall -y transformers\n",
        "!pip install \"transformers>=4.46.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV6QS2fVGgFd",
        "outputId": "5199de35-0f47-40c5-bc26-b8a2013e493e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: codecarbon in /usr/local/lib/python3.12/dist-packages (3.0.7)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.12/dist-packages (from codecarbon) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from codecarbon) (8.3.0)\n",
            "Requirement already satisfied: fief-client[cli] in /usr/local/lib/python3.12/dist-packages (from codecarbon) (0.20.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from codecarbon) (2.2.2)\n",
            "Requirement already satisfied: prometheus_client in /usr/local/lib/python3.12/dist-packages (from codecarbon) (0.23.1)\n",
            "Requirement already satisfied: psutil>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from codecarbon) (7.1.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from codecarbon) (9.0.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from codecarbon) (2.11.10)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from codecarbon) (12.575.51)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (from codecarbon) (3.14.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from codecarbon) (2.32.4)\n",
            "Requirement already satisfied: questionary in /usr/local/lib/python3.12/dist-packages (from codecarbon) (2.1.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from codecarbon) (0.19.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from arrow->codecarbon) (2.9.0.post0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.12/dist-packages (from arrow->codecarbon) (2.9.0.20251008)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from fief-client[cli]->codecarbon) (0.27.2)\n",
            "Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in /usr/local/lib/python3.12/dist-packages (from fief-client[cli]->codecarbon) (1.5.6)\n",
            "Requirement already satisfied: yaspin in /usr/local/lib/python3.12/dist-packages (from fief-client[cli]->codecarbon) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->codecarbon) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->codecarbon) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->codecarbon) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->codecarbon) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->codecarbon) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->codecarbon) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->codecarbon) (0.4.2)\n",
            "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from questionary->codecarbon) (3.0.52)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->codecarbon) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->codecarbon) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->codecarbon) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->codecarbon) (2025.10.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->codecarbon) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->codecarbon) (2.19.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.16.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.12/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.17.0)\n",
            "Requirement already satisfied: termcolor<4.0,>=3.1 in /usr/local/lib/python3.12/dist-packages (from yaspin->fief-client[cli]->codecarbon) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.23)\n",
            "Found existing installation: transformers 4.57.1\n",
            "Uninstalling transformers-4.57.1:\n",
            "  Successfully uninstalled transformers-4.57.1\n",
            "Collecting transformers>=4.46.0\n",
            "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (2025.10.5)\n",
            "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.57.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OraiHRVkEhEJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Optional\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    GenerationConfig,\n",
        "    PreTrainedModel,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from codecarbon import EmissionsTracker\n",
        "except ImportError:  # pragma: no cover - only hit when codecarbon missing\n",
        "    EmissionsTracker = None"
      ],
      "metadata": {
        "id": "B-CT6shnGfVS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEACHER_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "STUDENT_MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "DISTILLATION_DATASET_PATH = Path(\"distillation_dataset.jsonl\")\n",
        "DISTILLED_MODEL_DIR = Path(\"llama3_2_1b_docstring_distilled\")\n",
        "DEFAULT_NUM_DISTILLATION_SAMPLES = 256\n",
        "MAX_SOURCE_LENGTH = 2048\n",
        "MAX_TARGET_LENGTH = 512\n",
        "MAX_CLEAN_LINES = 60\n",
        "MIN_DOC_MARKER = (\"Args:\", \"Parameters:\", \"Returns:\")\n",
        "CODE_BLOCK_PATTERN = re.compile(r\"```(?:python)?\\s*([\\s\\S]*?)```\", re.IGNORECASE)\n",
        "DEFAULT_SUMMARY = \"Detects if the account balance drops below zero during the provided operations.\"\n",
        "CANONICAL_TEMPLATE = \"\"\"from typing import List\n",
        "\n",
        "def below_zero(operations: List[int]) -> bool:\n",
        "    \\\"\\\"\\\"\n",
        "    {summary}\n",
        "\n",
        "    Args:\n",
        "        operations (List[int]): A list of deposit and withdrawal operations.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the balance falls below zero, False otherwise.\n",
        "    \\\"\\\"\\\"\n",
        "    balance = 0\n",
        "    for op in operations:\n",
        "        balance += op\n",
        "        if balance < 0:\n",
        "            return True\n",
        "    return False\n",
        "\"\"\"\n",
        "function_code = \"\"\"\n",
        "from typing import List\n",
        "\n",
        "def below_zero(operations: List[int]) -> bool:\n",
        "    \\\"\\\"\\\"\n",
        "    You're given a list of deposit and withdrawal operations\n",
        "    on a bank account that starts with zero balance. Your task is to\n",
        "    detect if at any point the balance of account falls below zero,\n",
        "    and at that point function should return True.\n",
        "    \\\"\\\"\\\"\n",
        "    balance = 0\n",
        "    for op in operations:\n",
        "        balance += op\n",
        "        if balance < 0:\n",
        "            return True\n",
        "    return False\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "complete_prompt_text = f\"\"\"\n",
        "Please act as an expert Python software engineer. Given the python function below:\n",
        "{function_code}\n",
        "I would appreciate it if you could generate a complete and professional Google-style docstring.\n",
        "The docstring should not include any extra commentary, strictly limited to include the docstring itself and the original function code.\n",
        "CODE ONLY. Use standard Python indentation. Thank you.\n",
        "Do not add explanations, notes, or text outside of the code.\n",
        "Return only the function code with its docstring, without markdown fences or extra text before or after.\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "tWoyjw52GmG-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_pad_token(\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    model: Optional[PreTrainedModel] = None,\n",
        "    return_model: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Ensure tokenizer has a distinct <pad> token.\n",
        "    If 'model' is provided and we add a new token, resize embeddings and\n",
        "    mirror pad_token_id into model.config.\n",
        "    - return_model=False (default): returns tokenizer\n",
        "    - return_model=True: returns (tokenizer, model)\n",
        "    \"\"\"\n",
        "    added = False\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
        "        added = True\n",
        "\n",
        "    if model is not None:\n",
        "        if added:\n",
        "            model.resize_token_embeddings(len(tokenizer))\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    if return_model:\n",
        "        return tokenizer, model\n",
        "    return tokenizer\n",
        "# For TRAINING the student (you MAY add a new <pad> and must resize):\n",
        "def ensure_pad_for_training(tokenizer, model):\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    return tokenizer, model\n",
        "\n",
        "# For INFERENCE with TEACHER (DO NOT change vocab; alias pad→eos):\n",
        "def ensure_pad_for_inference(tokenizer, model):\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    return tokenizer, model\n",
        "@dataclass\n",
        "class PromptResponseRecord:\n",
        "    prompt: str\n",
        "    response: str\n",
        "\n",
        "\n",
        "class PromptResponseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Converts prompt/response pairs into autoregressive training examples.\n",
        "    The prompt tokens are masked in the label tensor so that the loss only\n",
        "    supervises the generated answer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        records: Iterable[PromptResponseRecord],\n",
        "        max_source_length: int = MAX_SOURCE_LENGTH,\n",
        "        max_target_length: int = MAX_TARGET_LENGTH,\n",
        "    ) -> None:\n",
        "        self.tokenizer = tokenizer\n",
        "        self.records = list(records)\n",
        "        self.max_source_length = max_source_length\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Dict[str, List[int]]:\n",
        "        record = self.records[index]\n",
        "        source_encoding = self.tokenizer(\n",
        "            record.prompt,\n",
        "            truncation=True,\n",
        "            max_length=self.max_source_length,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "        target_encoding = self.tokenizer(\n",
        "            record.response,\n",
        "            truncation=True,\n",
        "            max_length=self.max_target_length,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "\n",
        "        input_ids = (\n",
        "            source_encoding[\"input_ids\"]\n",
        "            + target_encoding[\"input_ids\"]\n",
        "            + [self.tokenizer.eos_token_id]\n",
        "        )\n",
        "\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        labels = (\n",
        "            [-100] * len(source_encoding[\"input_ids\"])\n",
        "            + target_encoding[\"input_ids\"]\n",
        "            + [self.tokenizer.eos_token_id]\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "        }\n",
        "\n",
        "\n",
        "def pad_batch(batch: List[Dict[str, List[int]]], pad_token_id: int) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Pads a list of dicts containing tokenized samples into a uniform tensor batch.\n",
        "    \"\"\"\n",
        "    max_length = max(len(item[\"input_ids\"]) for item in batch)\n",
        "    input_ids: List[List[int]] = []\n",
        "    attention_masks: List[List[int]] = []\n",
        "    labels: List[List[int]] = []\n",
        "\n",
        "    for item in batch:\n",
        "        pad_length = max_length - len(item[\"input_ids\"])\n",
        "        input_ids.append(item[\"input_ids\"] + [pad_token_id] * pad_length)\n",
        "        attention_masks.append(item[\"attention_mask\"] + [0] * pad_length)\n",
        "        labels.append(item[\"labels\"] + [-100] * pad_length)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "        \"attention_mask\": torch.tensor(attention_masks, dtype=torch.long),\n",
        "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
        "    }\n",
        "\n",
        "\n",
        "def load_distillation_records(path: Path) -> List[PromptResponseRecord]:\n",
        "    if not path.exists():\n",
        "        return []\n",
        "    records: List[PromptResponseRecord] = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as fp:\n",
        "        for line in fp:\n",
        "            payload = json.loads(line)\n",
        "            records.append(\n",
        "                PromptResponseRecord(\n",
        "                    prompt=payload[\"prompt\"],\n",
        "                    response=payload[\"response\"],\n",
        "                )\n",
        "            )\n",
        "    return records\n",
        "\n",
        "\n",
        "def save_distillation_records(path: Path, records: Iterable[PromptResponseRecord]) -> None:\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as fp:\n",
        "        for record in records:\n",
        "            json.dump({\"prompt\": record.prompt, \"response\": record.response}, fp)\n",
        "            fp.write(\"\\n\")\n",
        "\n",
        "\n",
        "def _select_candidate_block(blocks: List[str]) -> Optional[str]:\n",
        "    \"\"\"Pick the best code block, preferring ones that already contain a docstring.\"\"\"\n",
        "    for block in reversed(blocks):\n",
        "        candidate = block.strip()\n",
        "        if any(marker in candidate for marker in MIN_DOC_MARKER):\n",
        "            return candidate\n",
        "    return blocks[-1].strip() if blocks else None\n",
        "\n",
        "\n",
        "def _deduplicate_lines(lines: List[str]) -> List[str]:\n",
        "    deduped: List[str] = []\n",
        "    for line in lines:\n",
        "        stripped = line.rstrip()\n",
        "        if not deduped or stripped != deduped[-1]:\n",
        "            deduped.append(stripped)\n",
        "    return deduped\n",
        "\n",
        "\n",
        "def clean_teacher_response(raw_response: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalizes teacher outputs by keeping only a single cleaned Python code block.\n",
        "    Falls back to a canonical docstring if the teacher output is malformed.\n",
        "    \"\"\"\n",
        "    raw_response = raw_response.replace(\"\\r\\n\", \"\\n\")\n",
        "    matches = CODE_BLOCK_PATTERN.findall(raw_response)\n",
        "    candidate = _select_candidate_block(matches)\n",
        "\n",
        "    if not candidate:\n",
        "        candidate = raw_response.strip()\n",
        "\n",
        "    lines = candidate.splitlines()\n",
        "    lines = _deduplicate_lines(lines)[:MAX_CLEAN_LINES]\n",
        "\n",
        "    cleaned = \"\\n\".join(lines).strip()\n",
        "    if not cleaned:\n",
        "        cleaned = CANONICAL_TEMPLATE.format(summary=DEFAULT_SUMMARY)\n",
        "\n",
        "    return cleaned if cleaned.endswith(\"\\n\") else cleaned + \"\\n\"\n",
        "\n",
        "\n",
        "def collect_teacher_generations(\n",
        "    prompt_text: str,\n",
        "    teacher_model_id: str,\n",
        "    num_samples: int = 4,\n",
        "    temperature: float = 0.7,\n",
        "    top_p: float = 0.95,\n",
        "    max_new_tokens: int = 128,\n",
        "    device: str = \"cuda\",\n",
        "):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(teacher_model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        teacher_model_id,\n",
        "        torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),\n",
        "        low_cpu_mem_usage=True,\n",
        "    ).to(device).eval()\n",
        "\n",
        "    print(\"len(tokenizer), config.vocab_size, emb_size =\",\n",
        "      len(tokenizer),\n",
        "      getattr(model.config, \"vocab_size\", None),\n",
        "      model.get_input_embeddings().weight.shape[0])\n",
        "\n",
        "    # IMPORTANT: do NOT change teacher vocab size\n",
        "    tokenizer, model = ensure_pad_for_inference(tokenizer, model)\n",
        "\n",
        "    # Encode WITHOUT auto BOS/EOS (avoids double-BOS on Llama 3.x)\n",
        "    encoded = tokenizer(prompt_text, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
        "\n",
        "    teacher_gen_cfg = GenerationConfig(\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    outs = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_samples):\n",
        "            gen = model.generate(\n",
        "                **encoded,\n",
        "                generation_config=teacher_gen_cfg,\n",
        "                use_model_defaults=False,  # prevent baked-in overrides (e.g., do_sample/top_p)\n",
        "            )\n",
        "            prompt_len = encoded[\"input_ids\"].shape[1]\n",
        "            new_tokens = gen[0][prompt_len:]\n",
        "            outs.append(tokenizer.decode(new_tokens, skip_special_tokens=True))\n",
        "    return outs\n",
        "\n",
        "def _clean_resp(x: str) -> str:\n",
        "    try:\n",
        "        return clean_teacher_response(x)  # noqa: F821\n",
        "    except NameError:\n",
        "        return x\n",
        "\n",
        "def prepare_distillation_dataset(\n",
        "    prompt_text: str,\n",
        "    dataset_path: Path = DISTILLATION_DATASET_PATH,\n",
        "    teacher_model_id: str = TEACHER_MODEL_ID,\n",
        "    force_regenerate: bool = False,\n",
        "    num_samples: int = DEFAULT_NUM_DISTILLATION_SAMPLES,\n",
        "    temperature: float = 0.7,\n",
        "    top_p: float = 0.95,\n",
        "    max_new_tokens: int = MAX_TARGET_LENGTH,\n",
        "    device: str = \"cuda\",\n",
        ") -> List[PromptResponseRecord]:\n",
        "    \"\"\"\n",
        "    Loads cached teacher generations if present (unless force_regenerate),\n",
        "    otherwise collects fresh generations from the teacher, normalizes, and saves JSONL.\n",
        "    \"\"\"\n",
        "    # 1) Load cached if available\n",
        "    if dataset_path.exists() and not force_regenerate:\n",
        "        print(f\"Loading cached distillation dataset from {dataset_path}\")\n",
        "        records = load_distillation_records(dataset_path)\n",
        "        cleaned_records = [\n",
        "            PromptResponseRecord(prompt=r.prompt, response=_clean_resp(r.response))\n",
        "            for r in records\n",
        "        ]\n",
        "        save_distillation_records(dataset_path, cleaned_records)\n",
        "        return cleaned_records\n",
        "\n",
        "    # 2) Collect from teacher\n",
        "    print(\n",
        "        f\"Collecting {num_samples} teacher generations from {teacher_model_id} \"\n",
        "        f\"using provided prompt.\"\n",
        "    )\n",
        "    teacher_outputs = collect_teacher_generations(\n",
        "        prompt_text=prompt_text,\n",
        "        teacher_model_id=teacher_model_id,\n",
        "        num_samples=num_samples,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # 3) Normalize to records (PromptResponseRecord list)\n",
        "    records = [\n",
        "        PromptResponseRecord(prompt=prompt_text, response=_clean_resp(o))\n",
        "        for o in teacher_outputs\n",
        "    ]\n",
        "\n",
        "    # 4) Save to both the canonical dataset_path and a flat file for inspection\n",
        "    dataset_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    save_distillation_records(dataset_path, records)\n",
        "    save_distillation_records(Path(\"distillation_dataset.jsonl\"), records)\n",
        "\n",
        "    print(f\"Saved distillation dataset to {dataset_path}\")\n",
        "    return records\n",
        "\n",
        "\n",
        "# ---------- 1) Train / Distill ----------\n",
        "\n",
        "def distill_student_from_records(\n",
        "    records: List[PromptResponseRecord],\n",
        "    student_model_id: str = STUDENT_MODEL_ID,\n",
        "    output_dir: Path = DISTILLED_MODEL_DIR,\n",
        "    num_train_epochs: int = 3,\n",
        "    per_device_batch_size: int = 1,\n",
        "    learning_rate: float = 5e-5,\n",
        "    gradient_accumulation_steps: int = 2,\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Fine-tunes the student model on the teacher-generated dataset (naive distillation).\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- Load in FP32 for T4 training to avoid AMP/GradScaler issues ----\n",
        "    tokenizer = AutoTokenizer.from_pretrained(student_model_id, use_fast=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        student_model_id,\n",
        "        torch_dtype=torch.float32,      # train in FP32 on T4\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "\n",
        "    # Add <pad> if needed and resize embeddings\n",
        "    tokenizer, model = ensure_pad_for_training(tokenizer, model)  # from earlier message\n",
        "\n",
        "    # Memory savers\n",
        "    if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "        model.gradient_checkpointing_enable()  # big saver on T4\n",
        "    model.config.use_cache = False             # must be False when grad ckpt is on\n",
        "\n",
        "    # Build dataset AFTER tokenizer finalized\n",
        "    dataset = PromptResponseDataset(tokenizer, records)\n",
        "\n",
        "    from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "        pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=str(output_dir),\n",
        "        per_device_train_batch_size=per_device_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        logging_steps=50,\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        report_to=[],\n",
        "\n",
        "        # ---- IMPORTANT: no mixed precision (prevents GradScaler.unscale_ path) ----\n",
        "        fp16=False,\n",
        "        bf16=False,\n",
        "\n",
        "        # Optional memory/throughput tuners (safe defaults)\n",
        "        optim=\"adamw_torch\",\n",
        "        gradient_checkpointing=True,  # mirrors the manual enable; redundant but explicit\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Re-enable cache for inference\n",
        "    model.config.use_cache = True\n",
        "\n",
        "    # Save model + tokenizer\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    return output_dir\n",
        "\n",
        "# ---------- 2) Inference (single, corrected version) ----------\n",
        "\n",
        "def generate_with_distilled_model(\n",
        "    prompt_text: str,\n",
        "    model_path: Path = DISTILLED_MODEL_DIR,\n",
        "    max_new_tokens: int = MAX_TARGET_LENGTH,\n",
        "    temperature: float = 0.9,\n",
        "    top_p: float = 0.95,\n",
        "    do_sample: bool = True,\n",
        "    seed: Optional[int] = None,\n",
        "    model: Optional[torch.nn.Module] = None,\n",
        "    tokenizer: Optional[PreTrainedTokenizer] = None,\n",
        "    device: Optional[torch.device] = None,\n",
        ") -> str:\n",
        "    if model is None and not model_path.exists():\n",
        "        raise FileNotFoundError(f\"Distilled model directory '{model_path}' not found.\")\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "    if tokenizer is None:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    if model is None:\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch_dtype, low_cpu_mem_usage=True)\n",
        "\n",
        "    # Student path: tokenizer may have added <pad>; ensure model matches\n",
        "    tokenizer, model = ensure_pad_for_training(tokenizer, model)\n",
        "\n",
        "    model.to(device).eval()\n",
        "    model.config.use_cache = True\n",
        "\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(int(seed))\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(int(seed))\n",
        "\n",
        "    encoded = tokenizer(prompt_text, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**encoded)\n",
        "        last_logits = out.logits[:, -1, :]\n",
        "        if torch.isnan(last_logits).any() or torch.isinf(last_logits).any():\n",
        "            raise RuntimeError(\"Invalid logits (NaN/Inf) before generation – tokenizer/model mismatch?\")\n",
        "\n",
        "    gen_cfg = GenerationConfig(\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        temperature=temperature if do_sample else None,\n",
        "        top_p=top_p if do_sample else None,\n",
        "        num_beams=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen = model.generate(\n",
        "            **encoded,\n",
        "            generation_config=gen_cfg,\n",
        "            use_model_defaults=False,\n",
        "        )\n",
        "\n",
        "    prompt_len = encoded[\"input_ids\"].shape[1]\n",
        "    new_tokens = gen[0][prompt_len:]\n",
        "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def get_model_size_mb(model: torch.nn.Module) -> float:\n",
        "    torch.save(model.state_dict(), \"temp_model.pt\")\n",
        "    size_mb = os.path.getsize(\"temp_model.pt\") / (1024 * 1024)\n",
        "    os.remove(\"temp_model.pt\")\n",
        "    return size_mb   # Convert MB to GB for reporting\n",
        "\n",
        "\n",
        "def run_distilled_inference_trials(\n",
        "    prompt_text: str,\n",
        "    model_path: Path = DISTILLED_MODEL_DIR,\n",
        "    num_runs: int = 5,\n",
        "    max_new_tokens: int = MAX_TARGET_LENGTH,\n",
        "    temperature: float = 0.9,\n",
        "    top_p: float = 0.95,\n",
        "    base_seed: Optional[int] = None,\n",
        "    metrics_csv: Path = Path(\"distilled_inference_metrics.csv\"),\n",
        "    metadata_csv: Path = Path(\"distilled_model_metadata.csv\"),\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Executes multiple inference runs with the distilled model while monitoring emissions.\n",
        "    \"\"\"\n",
        "    tokenizer = ensure_pad_token(AutoTokenizer.from_pretrained(model_path))\n",
        "    torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        dtype=torch_dtype,\n",
        "    )\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    model_size_mb = get_model_size_mb(model)\n",
        "    if base_seed is None:\n",
        "        base_seed = (time.time_ns() & 0xFFFFFFFF)\n",
        "    rows: List[Dict[str, object]] = []\n",
        "    if EmissionsTracker is None:\n",
        "        print(\"CodeCarbon not installed; emissions data will be None.\")\n",
        "\n",
        "    for run_id in range(1, num_runs + 1):\n",
        "        run_seed = int(base_seed + 9973 * run_id)\n",
        "        torch.manual_seed(run_seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(run_seed)\n",
        "        tracker = None\n",
        "        emissions_kg = None\n",
        "\n",
        "        if EmissionsTracker is not None:\n",
        "            tracker = EmissionsTracker(save_to_file=False, measure_power_secs=1)\n",
        "            tracker.start()\n",
        "\n",
        "        # Logit sanity probe (last-step forward)\n",
        "        with torch.no_grad():\n",
        "            encoded = tokenizer(prompt_text, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
        "            out = model(**encoded)\n",
        "            last_logits = out.logits[:, -1, :]\n",
        "            if torch.isnan(last_logits).any() or torch.isinf(last_logits).any():\n",
        "                raise RuntimeError(\"Invalid logits (NaN/Inf) before generation – check tokenizer/model alignment.\")\n",
        "\n",
        "        # Dimensions must align\n",
        "        vocab = model.get_input_embeddings().weight.shape[0]\n",
        "        assert vocab == model.config.vocab_size or model.config.vocab_size is None, \\\n",
        "            f\"Embedding size {vocab} != config.vocab_size {model.config.vocab_size}\"\n",
        "        assert tokenizer.pad_token_id is not None, \"pad_token_id must be set\"\n",
        "        start = time.perf_counter()\n",
        "        output = generate_with_distilled_model(\n",
        "            prompt_text=prompt_text,\n",
        "            model_path=model_path,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,        # ensure sampling\n",
        "            seed=None,             # seeding handled globally above\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=device,\n",
        "        )\n",
        "        inference_time = time.perf_counter() - start\n",
        "\n",
        "        if tracker is not None:\n",
        "            emissions_kg = tracker.stop()\n",
        "\n",
        "        rows.append(\n",
        "            {\n",
        "                \"run_id\": run_id,\n",
        "                \"prompt\": prompt_text,\n",
        "                \"output\": output,\n",
        "                \"accuracy\": \"N/A\",\n",
        "                \"inference_time_s\": inference_time,\n",
        "                \"emissions_kg\": emissions_kg,\n",
        "                \"model_size_mb\": model_size_mb,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    metrics_df = pd.DataFrame(rows)\n",
        "    metrics_df.to_csv(metrics_csv, index=False)\n",
        "\n",
        "    metadata_df = pd.DataFrame(\n",
        "        [\n",
        "            {\n",
        "                \"model_path\": str(model_path),\n",
        "                \"model_size_mb\": model_size_mb,\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    metadata_df.to_csv(metadata_csv, index=False)\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "\n",
        "def run_distillation_workflow(\n",
        "    prompt_text: str,\n",
        "    teacher_model_id: str = TEACHER_MODEL_ID,\n",
        "    student_model_id: str = STUDENT_MODEL_ID,\n",
        "    dataset_path: Path = DISTILLATION_DATASET_PATH,\n",
        "    distilled_dir: Path = DISTILLED_MODEL_DIR,\n",
        "    evaluation_runs: int = 5,\n",
        "    force_regenerate: bool = False,\n",
        "    metrics_csv: Path = Path(\"distilled_inference_metrics.csv\"),\n",
        "    metadata_csv: Path = Path(\"distilled_model_metadata.csv\"),\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    End-to-end pipeline that prepares the dataset, fine-tunes the student,\n",
        "    and returns the distilled model output for the provided prompt.\n",
        "    \"\"\"\n",
        "    records = prepare_distillation_dataset(\n",
        "        prompt_text=prompt_text,\n",
        "        dataset_path=dataset_path,\n",
        "        teacher_model_id=teacher_model_id,\n",
        "        force_regenerate=force_regenerate,\n",
        "    )\n",
        "    print(f\"Distillation dataset contains {len(records)} samples.\")\n",
        "\n",
        "    distilled_path = distill_student_from_records(\n",
        "        records=records,\n",
        "        student_model_id=student_model_id,\n",
        "        output_dir=distilled_dir,\n",
        "    )\n",
        "    print(f\"Saved distilled student model to {distilled_path}\")\n",
        "\n",
        "    metrics_df = run_distilled_inference_trials(\n",
        "        prompt_text=prompt_text,\n",
        "        model_path=distilled_path,\n",
        "        num_runs=evaluation_runs,\n",
        "        metrics_csv=metrics_csv,\n",
        "        metadata_csv=metadata_csv,\n",
        "    )\n",
        "    if not metrics_df.empty:\n",
        "        sample = metrics_df.iloc[0][\"output\"]\n",
        "        print(\"=== Distilled Model Output (Run 1) ===\")\n",
        "        print(sample)\n",
        "        return str(sample)\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "b0VoIMY6ZfoH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_distillation_workflow(complete_prompt_text, force_regenerate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1-9U_8HWZnVP",
        "outputId": "d510b5eb-d11e-4ca3-ebe8-0ab0bf6d5612"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached distillation dataset from distillation_dataset.jsonl\n",
            "Distillation dataset contains 256 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [384/384 04:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.224500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.132800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.093000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.075800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.077400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.055900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.053000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved distilled student model to llama3_2_1b_docstring_distilled\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 00:29:32] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 00:29:32] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 00:29:32] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 00:29:33] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 00:29:33] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 00:29:33] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon WARNING @ 00:29:33] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon INFO @ 00:29:33] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 00:29:33] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 00:29:33] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: RAM power estimation model\n",
            "                CPU Tracking Method: global constant\n",
            "                GPU Tracking Method: pynvml\n",
            "            \n",
            "[codecarbon INFO @ 00:29:33] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 00:29:33]   Platform system: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 00:29:33]   Python version: 3.12.12\n",
            "[codecarbon INFO @ 00:29:33]   CodeCarbon version: 3.0.7\n",
            "[codecarbon INFO @ 00:29:33]   Available RAM : 83.474 GB\n",
            "[codecarbon INFO @ 00:29:33]   CPU count: 12 thread(s) in 1 physical CPU(s)\n",
            "[codecarbon INFO @ 00:29:33]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 00:29:33]   GPU count: 1\n",
            "[codecarbon INFO @ 00:29:33]   GPU model: 1 x NVIDIA A100-SXM4-40GB\n",
            "[codecarbon INFO @ 00:29:34] Energy consumed for RAM : 0.000011 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:34] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:34] Energy consumed for All CPU : 0.000012 kWh\n",
            "[codecarbon INFO @ 00:29:34] Energy consumed for all GPUs : 0.000018 kWh. Total GPU Power : 64.32653289459041 W\n",
            "[codecarbon INFO @ 00:29:34] 0.000040 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:35] Energy consumed for RAM : 0.000021 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:35] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:35] Energy consumed for All CPU : 0.000024 kWh\n",
            "[codecarbon INFO @ 00:29:35] Energy consumed for all GPUs : 0.000037 kWh. Total GPU Power : 66.93807438545593 W\n",
            "[codecarbon INFO @ 00:29:35] 0.000081 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:36] Energy consumed for RAM : 0.000032 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:36] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:36] Energy consumed for All CPU : 0.000035 kWh\n",
            "[codecarbon INFO @ 00:29:36] Energy consumed for all GPUs : 0.000055 kWh. Total GPU Power : 67.86707609136991 W\n",
            "[codecarbon INFO @ 00:29:36] 0.000122 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:37] Energy consumed for RAM : 0.000042 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:37] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:37] Energy consumed for All CPU : 0.000047 kWh\n",
            "[codecarbon INFO @ 00:29:37] Energy consumed for all GPUs : 0.000074 kWh. Total GPU Power : 68.11915400375428 W\n",
            "[codecarbon INFO @ 00:29:37] 0.000163 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:37] Energy consumed for RAM : 0.000044 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:37] Delta energy consumed for CPU with constant : 0.000002 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:37] Energy consumed for All CPU : 0.000049 kWh\n",
            "[codecarbon INFO @ 00:29:37] Energy consumed for all GPUs : 0.000076 kWh. Total GPU Power : 43.926810967623666 W\n",
            "[codecarbon INFO @ 00:29:37] 0.000169 kWh of electricity used since the beginning.\n",
            "[codecarbon WARNING @ 00:29:37] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 00:29:37] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 00:29:37] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 00:29:38] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 00:29:38] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 00:29:38] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon WARNING @ 00:29:38] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon INFO @ 00:29:38] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 00:29:38] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 00:29:38] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: RAM power estimation model\n",
            "                CPU Tracking Method: global constant\n",
            "                GPU Tracking Method: pynvml\n",
            "            \n",
            "[codecarbon INFO @ 00:29:38] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 00:29:38]   Platform system: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 00:29:38]   Python version: 3.12.12\n",
            "[codecarbon INFO @ 00:29:38]   CodeCarbon version: 3.0.7\n",
            "[codecarbon INFO @ 00:29:38]   Available RAM : 83.474 GB\n",
            "[codecarbon INFO @ 00:29:38]   CPU count: 12 thread(s) in 1 physical CPU(s)\n",
            "[codecarbon INFO @ 00:29:38]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 00:29:38]   GPU count: 1\n",
            "[codecarbon INFO @ 00:29:38]   GPU model: 1 x NVIDIA A100-SXM4-40GB\n",
            "[codecarbon INFO @ 00:29:39] Energy consumed for RAM : 0.000011 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:39] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:39] Energy consumed for All CPU : 0.000012 kWh\n",
            "[codecarbon INFO @ 00:29:39] Energy consumed for all GPUs : 0.000019 kWh. Total GPU Power : 68.38113465045058 W\n",
            "[codecarbon INFO @ 00:29:39] 0.000042 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:40] Energy consumed for RAM : 0.000021 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:40] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:40] Energy consumed for All CPU : 0.000024 kWh\n",
            "[codecarbon INFO @ 00:29:40] Energy consumed for all GPUs : 0.000038 kWh. Total GPU Power : 67.15763268051712 W\n",
            "[codecarbon INFO @ 00:29:40] 0.000082 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:41] Energy consumed for RAM : 0.000032 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:41] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:41] Energy consumed for All CPU : 0.000035 kWh\n",
            "[codecarbon INFO @ 00:29:41] Energy consumed for all GPUs : 0.000056 kWh. Total GPU Power : 67.51270588050397 W\n",
            "[codecarbon INFO @ 00:29:41] 0.000123 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:42] Energy consumed for RAM : 0.000034 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:42] Delta energy consumed for CPU with constant : 0.000002 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:42] Energy consumed for All CPU : 0.000038 kWh\n",
            "[codecarbon INFO @ 00:29:42] Energy consumed for all GPUs : 0.000058 kWh. Total GPU Power : 34.30489201595377 W\n",
            "[codecarbon INFO @ 00:29:42] 0.000130 kWh of electricity used since the beginning.\n",
            "[codecarbon WARNING @ 00:29:42] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 00:29:42] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 00:29:42] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 00:29:43] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 00:29:43] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 00:29:43] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon WARNING @ 00:29:43] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon INFO @ 00:29:43] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 00:29:43] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 00:29:43] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: RAM power estimation model\n",
            "                CPU Tracking Method: global constant\n",
            "                GPU Tracking Method: pynvml\n",
            "            \n",
            "[codecarbon INFO @ 00:29:43] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 00:29:43]   Platform system: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 00:29:43]   Python version: 3.12.12\n",
            "[codecarbon INFO @ 00:29:43]   CodeCarbon version: 3.0.7\n",
            "[codecarbon INFO @ 00:29:43]   Available RAM : 83.474 GB\n",
            "[codecarbon INFO @ 00:29:43]   CPU count: 12 thread(s) in 1 physical CPU(s)\n",
            "[codecarbon INFO @ 00:29:43]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 00:29:43]   GPU count: 1\n",
            "[codecarbon INFO @ 00:29:43]   GPU model: 1 x NVIDIA A100-SXM4-40GB\n",
            "[codecarbon INFO @ 00:29:44] Energy consumed for RAM : 0.000011 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:44] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:44] Energy consumed for All CPU : 0.000012 kWh\n",
            "[codecarbon INFO @ 00:29:44] Energy consumed for all GPUs : 0.000019 kWh. Total GPU Power : 67.8473029851074 W\n",
            "[codecarbon INFO @ 00:29:44] 0.000041 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:45] Energy consumed for RAM : 0.000021 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:45] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:45] Energy consumed for All CPU : 0.000024 kWh\n",
            "[codecarbon INFO @ 00:29:45] Energy consumed for all GPUs : 0.000038 kWh. Total GPU Power : 67.99056997623163 W\n",
            "[codecarbon INFO @ 00:29:45] 0.000082 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:46] Energy consumed for RAM : 0.000030 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:46] Delta energy consumed for CPU with constant : 0.000010 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:46] Energy consumed for All CPU : 0.000034 kWh\n",
            "[codecarbon INFO @ 00:29:46] Energy consumed for all GPUs : 0.000055 kWh. Total GPU Power : 69.52581890136025 W\n",
            "[codecarbon INFO @ 00:29:46] 0.000119 kWh of electricity used since the beginning.\n",
            "[codecarbon WARNING @ 00:29:46] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 00:29:46] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 00:29:46] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 00:29:47] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 00:29:47] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 00:29:47] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon WARNING @ 00:29:47] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon INFO @ 00:29:47] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 00:29:47] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 00:29:47] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: RAM power estimation model\n",
            "                CPU Tracking Method: global constant\n",
            "                GPU Tracking Method: pynvml\n",
            "            \n",
            "[codecarbon INFO @ 00:29:47] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 00:29:47]   Platform system: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 00:29:47]   Python version: 3.12.12\n",
            "[codecarbon INFO @ 00:29:47]   CodeCarbon version: 3.0.7\n",
            "[codecarbon INFO @ 00:29:47]   Available RAM : 83.474 GB\n",
            "[codecarbon INFO @ 00:29:47]   CPU count: 12 thread(s) in 1 physical CPU(s)\n",
            "[codecarbon INFO @ 00:29:47]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 00:29:47]   GPU count: 1\n",
            "[codecarbon INFO @ 00:29:47]   GPU model: 1 x NVIDIA A100-SXM4-40GB\n",
            "[codecarbon INFO @ 00:29:48] Energy consumed for RAM : 0.000011 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:48] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:48] Energy consumed for All CPU : 0.000012 kWh\n",
            "[codecarbon INFO @ 00:29:48] Energy consumed for all GPUs : 0.000019 kWh. Total GPU Power : 67.39455049518176 W\n",
            "[codecarbon INFO @ 00:29:48] 0.000041 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:49] Energy consumed for RAM : 0.000021 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:49] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:49] Energy consumed for All CPU : 0.000024 kWh\n",
            "[codecarbon INFO @ 00:29:49] Energy consumed for all GPUs : 0.000038 kWh. Total GPU Power : 67.62222812908921 W\n",
            "[codecarbon INFO @ 00:29:49] 0.000082 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:50] Energy consumed for RAM : 0.000032 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:50] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:50] Energy consumed for All CPU : 0.000035 kWh\n",
            "[codecarbon INFO @ 00:29:50] Energy consumed for all GPUs : 0.000056 kWh. Total GPU Power : 67.0014481555541 W\n",
            "[codecarbon INFO @ 00:29:50] 0.000123 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:50] Energy consumed for RAM : 0.000034 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:50] Delta energy consumed for CPU with constant : 0.000003 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:50] Energy consumed for All CPU : 0.000038 kWh\n",
            "[codecarbon INFO @ 00:29:50] Energy consumed for all GPUs : 0.000062 kWh. Total GPU Power : 81.83455394187946 W\n",
            "[codecarbon INFO @ 00:29:50] 0.000134 kWh of electricity used since the beginning.\n",
            "[codecarbon WARNING @ 00:29:50] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 00:29:50] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 00:29:50] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 00:29:51] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 00:29:51] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 00:29:51] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon WARNING @ 00:29:51] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon INFO @ 00:29:51] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 00:29:51] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 00:29:51] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: RAM power estimation model\n",
            "                CPU Tracking Method: global constant\n",
            "                GPU Tracking Method: pynvml\n",
            "            \n",
            "[codecarbon INFO @ 00:29:51] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 00:29:51]   Platform system: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 00:29:51]   Python version: 3.12.12\n",
            "[codecarbon INFO @ 00:29:51]   CodeCarbon version: 3.0.7\n",
            "[codecarbon INFO @ 00:29:51]   Available RAM : 83.474 GB\n",
            "[codecarbon INFO @ 00:29:51]   CPU count: 12 thread(s) in 1 physical CPU(s)\n",
            "[codecarbon INFO @ 00:29:51]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 00:29:51]   GPU count: 1\n",
            "[codecarbon INFO @ 00:29:51]   GPU model: 1 x NVIDIA A100-SXM4-40GB\n",
            "[codecarbon INFO @ 00:29:53] Energy consumed for RAM : 0.000011 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:53] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:53] Energy consumed for All CPU : 0.000012 kWh\n",
            "[codecarbon INFO @ 00:29:53] Energy consumed for all GPUs : 0.000019 kWh. Total GPU Power : 67.9421060991256 W\n",
            "[codecarbon INFO @ 00:29:53] 0.000041 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:54] Energy consumed for RAM : 0.000021 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:54] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:54] Energy consumed for All CPU : 0.000024 kWh\n",
            "[codecarbon INFO @ 00:29:54] Energy consumed for all GPUs : 0.000038 kWh. Total GPU Power : 67.79970512542324 W\n",
            "[codecarbon INFO @ 00:29:54] 0.000082 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 00:29:55] Energy consumed for RAM : 0.000032 kWh. RAM Power : 38.0 W\n",
            "[codecarbon INFO @ 00:29:55] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 00:29:55] Energy consumed for All CPU : 0.000035 kWh\n",
            "[codecarbon INFO @ 00:29:55] Energy consumed for all GPUs : 0.000056 kWh. Total GPU Power : 67.84825340730418 W\n",
            "[codecarbon INFO @ 00:29:55] 0.000123 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Distilled Model Output (Run 1) ===\n",
            "from typing import List\n",
            "\n",
            "def below_zero(operations: List[int]) -> bool:\n",
            "    \"\"\"\n",
            "    You're given a list of deposit and withdrawal operations\n",
            "    on a bank account that starts with zero balance. Your task is to\n",
            "    detect if at any point the balance of account falls below zero,\n",
            "    and at that point function should return True.\n",
            "\n",
            "    Args:\n",
            "        operations (List[int]): A list of deposit and withdrawal operations.\n",
            "            Positive numbers represent deposits, while negative numbers represent withdrawals.\n",
            "\n",
            "    Returns:\n",
            "        bool: True if the balance falls below zero at any point, False otherwise.\n",
            "\n",
            "    Examples:\n",
            "        >>> below_zero([1, -2, 3, -4])\n",
            "        False\n",
            "        >>> below_zero([-1, 2, -3])\n",
            "        True\n",
            "    \"\"\"\n",
            "    balance = 0\n",
            "    for op in operations:\n",
            "        balance += op\n",
            "        if balance < 0:\n",
            "            return True\n",
            "    return False\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from typing import List\\n\\ndef below_zero(operations: List[int]) -> bool:\\n    \"\"\"\\n    You\\'re given a list of deposit and withdrawal operations\\n    on a bank account that starts with zero balance. Your task is to\\n    detect if at any point the balance of account falls below zero,\\n    and at that point function should return True.\\n\\n    Args:\\n        operations (List[int]): A list of deposit and withdrawal operations.\\n            Positive numbers represent deposits, while negative numbers represent withdrawals.\\n\\n    Returns:\\n        bool: True if the balance falls below zero at any point, False otherwise.\\n\\n    Examples:\\n        >>> below_zero([1, -2, 3, -4])\\n        False\\n        >>> below_zero([-1, 2, -3])\\n        True\\n    \"\"\"\\n    balance = 0\\n    for op in operations:\\n        balance += op\\n        if balance < 0:\\n            return True\\n    return False\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88f7e381",
        "outputId": "7c340349-83ab-46c0-ed20-f56ba7a53645"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "metrics_csv = Path(\"distilled_inference_metrics.csv\")\n",
        "metadata_csv = Path(\"distilled_model_metadata.csv\")\n",
        "\n",
        "!cp \"{metrics_csv}\" \"/content/drive/My Drive/\"\n",
        "!cp \"{metadata_csv}\" \"/content/drive/My Drive/\"\n",
        "\n",
        "print(f\"Downloaded {metrics_csv} and {metadata_csv} to Google Drive.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Downloaded distilled_inference_metrics.csv and distilled_model_metadata.csv to Google Drive.\n"
          ]
        }
      ]
    }
  ]
}